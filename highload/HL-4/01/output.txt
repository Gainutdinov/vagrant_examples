Задание:
Подготовка окружения
1. Создайте 3 виртуальных машины (ноды) u18-hadoop{1,2,3} следующей конфигурации:
○ Операционная система: Ubuntu 18.04
○ Количество ядер: 2 CPU
○ Оперативная память: 2 GB
○ Виртуальный диск: 20 ГБ
2. Обновите операционную систему на созданных нодах.
3. Добавьте записи для всех нодов в файл /etc/hosts на всех нодах кластера.
4. Установите рекомендуемую версию Java на нодах u18-hadoop{1,2,3}.
5. Установите сервисы ssh и pdsh. Они необходимы для удаленного управления сервисами Hadoop и улучшенного управления ssh.
Установка последней стабильной версии Hadoop

```
https://www.howtoforge.com/how-to-install-and-configure-apache-hadoop-on-ubuntu-2004/
https://www.linode.com/docs/guides/how-to-install-and-set-up-hadoop-cluster/
```

1. Скачайте последнюю стабильную версию Hadoop.
Подготовка к запуску кластера Hadoop
1. Распакуйте скачаную версию Hadoop и настройте очень важную переменную HADOOP_HOME.
2. Отредактируйте файл etc/hadoop/hadoop-env.sh, указав путь к расположению установленной версии Java в переменной окружения JAVA_HOME.
3. Проверьте, если скрипт Hadoop работает. Ны выходе Вы должны получить текст подсказки с параметрами запуска скрипта.
Запуск Hadoop в полном распределенном режиме
1. Создайте следующие файлы кофигурации:
○ файл core-site.xml, который должен указывать файловую систему по умолчанию, имя NameNode ВМ, а так же порт, который будут использовать DataNodes для отправки heartbeat
○ файл hdfs-site.xml, который должен указывать одну реплику блоков, на которые HDFS делит сохраняемые файлы
○ сделать YARN планировщиком заданий и основным рабочим фреймом для функционирования MapReduce, добавив конфигурацию в файл mapred-site.xml
○ добавить конфигурацию YARN в файл etc/hadoop/yarn-site.xml
○ сообщить мастер-ноде о всех существующих в кластере рабочих нодах путем перечисления их имен в файле etc/hadoop/workers
○ настроить параметры распредления памяти для YARN и MapReduce в файлах etc/hadoop/yarn-site.xml и etc/hadoop/mapred-site.xml
2. Создайте ssh доступ между u18-hadoop{1,2,3} без пароля. Для этого сгенерируйте локально новый ключ RSA, либо используйте готовый и скопируйте содержимое в файл ключей.
3. Скопируйте конфигурацию из папки etc/hadoop на все рабочие ноды.
4. Выполните запуск Hadoop:
○ выполните форматирование файловой системы HDFS с помощью команды hdfs и ее параметров выполнения.

```
#On master node - host1
hdfs namenode -format
```
○ запустите сервисы Hadoop при помощи скрипта start-dfs.sh.

```
#On master node - host1
start-dfs.sh
hadoop@host1:~$ start-dfs.sh
Starting namenodes on [host1]
Starting datanodes
Starting secondary namenodes [host1]
hadoop@host1:~$ jps
16706 SecondaryNameNode
16438 NameNode
16856 Jps
hadoop@host2:~$ jps
14242 Jps
14056 DataNode
```

Примечание: "маленький" слоник записывает логистику выполнения по умолчанию в директорию $HADOOP_HOME/logs, которая может быть изменена установкой переменной окружения $HADOOP_LOG_DIR. Еще
Примечание: иногда команда start-all.sh выдает ошибку. Полезные ссылки помогут.
○ попробуйте просмотреть веб интерфейс сервиса NameNode, по умолчанию он находится здесь: http://localhost:9870/

```
1.png
```

○ создайте директории /user и /user/<username> файловой системы HDFS, необходимые для выполнения работ MapReduce, используя команду hdfs и ее параметров выполнения.

```
hadoop@host1:~$ hdfs dfs -mkdir -p /user
hadoop@host1:~$ hdfs dfs -mkdir -p /user/mgainutdinov
hadoop@host1:~$ hdfs dfs -ls /
Found 1 items
drwxr-xr-x   - hadoop supergroup          0 2022-06-26 09:11 /user
hadoop@host1:~$ hdfs dfs -ls /user/
Found 1 items
drwxr-xr-x   - hadoop supergroup          0 2022-06-26 09:11 /user/mgainutdinov
hadoop@host1:~$
```

○ скопируйте исходные файлы etc/hadoop/*.xml на HDFS в директорий input, предварительно создав его.

```
hadoop@host1:~$ hdfs dfs -mkdir -p /user/mgainutdinov/input
hadoop@host1:~$ hdfs dfs -put /usr/local/hadoop/etc/hadoop/*.xml /user/mgainutdinov/input
hadoop@host1:~$ hdfs dfs -ls  /user/mgainutdinov/input
hadoop@host1:~$ hdfs dfs -ls  /user/mgainutdinov/input
Found 10 items
-rw-r--r--   1 hadoop supergroup       9213 2022-06-26 09:33 /user/mgainutdinov/input/capacity-scheduler.xml
-rw-r--r--   1 hadoop supergroup        265 2022-06-26 09:33 /user/mgainutdinov/input/core-site.xml
-rw-r--r--   1 hadoop supergroup      11765 2022-06-26 09:33 /user/mgainutdinov/input/hadoop-policy.xml
-rw-r--r--   1 hadoop supergroup        683 2022-06-26 09:33 /user/mgainutdinov/input/hdfs-rbf-site.xml
-rw-r--r--   1 hadoop supergroup        400 2022-06-26 09:33 /user/mgainutdinov/input/hdfs-site.xml
-rw-r--r--   1 hadoop supergroup        620 2022-06-26 09:33 /user/mgainutdinov/input/httpfs-site.xml
-rw-r--r--   1 hadoop supergroup       3518 2022-06-26 09:33 /user/mgainutdinov/input/kms-acls.xml
-rw-r--r--   1 hadoop supergroup        682 2022-06-26 09:33 /user/mgainutdinov/input/kms-site.xml
-rw-r--r--   1 hadoop supergroup        912 2022-06-26 09:33 /user/mgainutdinov/input/mapred-site.xml
-rw-r--r--   1 hadoop supergroup        899 2022-06-26 09:33 /user/mgainutdinov/input/yarn-site.xml
```

○ попробуйте выполнить файлы примеров share/hadoop/mapreduce/hadoop-mapreduce-examples-3.2.1.jar при помощи команды hadoop и ее параметров.

```
hadoop@host1:/usr/local/hadoop$ start-yarn.sh
Starting resourcemanager
Starting nodemanagers
hadoop@host1:/usr/local/hadoop$
hadoop@host1:/usr/local/hadoop$ yarn node -list
2022-06-26 09:36:45,942 INFO client.DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at host1/192.168.53.101:8032
Total Nodes:2
         Node-Id             Node-State Node-Http-Address       Number-of-Running-Containers
     host2:35035                RUNNING        host2:8042                                  0
     host3:46189                RUNNING        host3:8042                                  0
hadoop@host1:/usr/local/hadoop$
hadoop@host1:/usr/local/hadoop$ yarn jar /usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.3.1.jar  --help
Unknown program '--help' chosen.
Valid program names are:
  aggregatewordcount: An Aggregate based map/reduce program that counts the words in the input files.
  aggregatewordhist: An Aggregate based map/reduce program that computes the histogram of the words in the input files.
  bbp: A map/reduce program that uses Bailey-Borwein-Plouffe to compute exact digits of Pi.
  dbcount: An example job that count the pageview counts from a database.
  distbbp: A map/reduce program that uses a BBP-type formula to compute exact bits of Pi.
  grep: A map/reduce program that counts the matches of a regex in the input.
  join: A job that effects a join over sorted, equally partitioned datasets
  multifilewc: A job that counts words from several files.
  pentomino: A map/reduce tile laying program to find solutions to pentomino problems.
  pi: A map/reduce program that estimates Pi using a quasi-Monte Carlo method.
  randomtextwriter: A map/reduce program that writes 10GB of random textual data per node.
  randomwriter: A map/reduce program that writes 10GB of random data per node.
  secondarysort: An example defining a secondary sort to the reduce.
  sort: A map/reduce program that sorts the data written by the random writer.
  sudoku: A sudoku solver.
  teragen: Generate data for the terasort
  terasort: Run the terasort
  teravalidate: Checking results of terasort
  wordcount: A map/reduce program that counts the words in the input files.
  wordmean: A map/reduce program that counts the average length of the words in the input files.
  wordmedian: A map/reduce program that counts the median length of the words in the input files.
  wordstandarddeviation: A map/reduce program that counts the standard deviation of the length of the words in the input files.
hadoop@host1:/usr/local/hadoop$
hadoop@host1:/usr/local/hadoop$ yarn jar /usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.3.1.jar  wordcount "/user/mgainutdinov/input/capacity-scheduler.xml" output
...
```

○ исследуйте файлы на выходе выполнения примеров, предварительно скопировав их на локальную систему при помощи команды hdfs.

```
hadoop@host1:/usr/local/hadoop$ hdfs dfs -cat  /user/hadoop/output/part-r-00000
"AS     1
"
...
hadoop@host1:/usr/local/hadoop$ hdfs dfs -get  /user/hadoop/output/part-r-00000

```
○ по окончании исследований остановите сервисы Hadoop при помощи скрипта stop-all.sh.

```
#On master node - host1
stop-all.sh
```

