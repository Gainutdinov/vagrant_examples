---
- name: hadoop install
  hosts: all
  become: yes
  gather_facts: yes
  vars:
    ssh_authorize_user_target_user: "hadoop"
    source_user_ssh_private_key: |
      -----BEGIN OPENSSH PRIVATE KEY-----
      b3BlbnNzaC1rZXktdjEAAAAABG5vbmUAAAAEbm9uZQAAAAAAAAABAAABFwAAAAdzc2gtcn
      NhAAAAAwEAAQAAAQEA72uGNtKpWksjojBR28X8bFcUNYCFjEX91UmywNmYh7OEeFAp4oWr
      b05HvRYqktiEkYs4eO7CQ+QpTADmbDbKww5EZ5GysjRxOiUnJKs9qqtnfQDfdtVp5s4nkf
      QghPWUvQSKRPs9pmvFGUX2RDNiUHXiWWtNJflnkGSU27shLosciuEMkW7Xp/a0RBG6PAKc
      wL0YXDZO8VwHdi7ekfhdBPgznlTYMgb5kCrZdKX2A0gYx5uPoP1XPru5dwaH+X7Laya1BY
      Dkf+RdqFy0S5MRfOzgvc/9qIYxSEgXh/NirfFOSpJzAFCRQHLp9Ar3chwquKLBvOj2aTQ4
      LHc9Uvnu6wAAA9AMVFvTDFRb0wAAAAdzc2gtcnNhAAABAQDva4Y20qlaSyOiMFHbxfxsVx
      Q1gIWMRf3VSbLA2ZiHs4R4UCnihatvTke9FiqS2ISRizh47sJD5ClMAOZsNsrDDkRnkbKy
      NHE6JSckqz2qq2d9AN921WnmzieR9CCE9ZS9BIpE+z2ma8UZRfZEM2JQdeJZa00l+WeQZJ
      TbuyEuixyK4QyRbten9rREEbo8ApzAvRhcNk7xXAd2Lt6R+F0E+DOeVNgyBvmQKtl0pfYD
      SBjHm4+g/Vc+u7l3Bof5fstrJrUFgOR/5F2oXLRLkxF87OC9z/2ohjFISBeH82Kt8U5Kkn
      MAUJFAcun0CvdyHCq4osG86PZpNDgsdz1S+e7rAAAAAwEAAQAAAQAIY0P4d7jnKOq+q+Ky
      SGVnhkV2+mGzYGBcMJDKoNTvLCLf9C8DeJtg8uUggr5LFrrOPopj8jlsF3o6520rLWA9JS
      rQVhJkm+G7n8GXT0iByi99+aHaj+993osUWfEpt1DWrmoJsKOgSPZGxGS0TviKyRZ8+31g
      bzRpopx0dkHbV4L22hxGMraDcmwIV15ZH1Il+zoW+WIi8E6SISer0PWE5pBlY7bqXnwbd0
      GvWg2a8ErNIfZly/1WlH6TpBJFl2o+T6uext5baciozHJQjI2vktyacQ8fRZ213G0KggVG
      YAYF2Xtj/uwf4A3uJEndfbnT1aoItAkfgTb20a5f2MxBAAAAgAZVQCj2Eby5TQWeNKWDig
      axrJRN4LCGSFVBXk6PlNNSO7mwgv0YyoscS++CCUMl72OxJUA8JCguvMlFq9JNHep1rDhq
      zL/teEgruo/jmrMslYqYANXE7Kt+0Nu0w3Sz4ZrC4/usveRNYrOY5VgxO+FMvYjxdyxF2G
      bXbyegWPDCAAAAgQD8XihZq6sX9qqKUBRtWeW9bEomgg93XkVhoClAwGyjj68yrYn+85qf
      OJAh/sjyFk4n3CJ4yDgk2yrzKmYJ52/hygAUh4ZAozpUV9vkVWyhhbm1TQ3ZGYkfnV1+aN
      NmVtja/7EMs+mPC8nycPkIbWgvHyUjdTKqIk78Kry9oHS6ZQAAAIEA8t2pL728+sTRILmh
      kmdHUrJmcvHZHY8/gsbjhS49+QcdmQeGk+HyUhjKT0DRwkEOtXGhuhtnMFGFfuzjn8fV2A
      sVdr3ssBhpW/sx+G+WIecVRLd8ZL6go5DPg9Ux2YI2SZs25FmGNveJ/fT/M4C8Fi9mqg7X
      GQn60BKas0MdRw8AAAAaYW5zaWJsZS1nZW5lcmF0ZWQgb24gaG9zdDEB
      -----END OPENSSH PRIVATE KEY-----
    source_user_ssh_public_key: "ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQDva4Y20qlaSyOiMFHbxfxsVxQ1gIWMRf3VSbLA2ZiHs4R4UCnihatvTke9FiqS2ISRizh47sJD5ClMAOZsNsrDDkRnkbKyNHE6JSckqz2qq2d9AN921WnmzieR9CCE9ZS9BIpE+z2ma8UZRfZEM2JQdeJZa00l+WeQZJTbuyEuixyK4QyRbten9rREEbo8ApzAvRhcNk7xXAd2Lt6R+F0E+DOeVNgyBvmQKtl0pfYDSBjHm4+g/Vc+u7l3Bof5fstrJrUFgOR/5F2oXLRLkxF87OC9z/2ohjFISBeH82Kt8U5KknMAUJFAcun0CvdyHCq4osG86PZpNDgsdz1S+e7r ansible-generated on host1"
  pre_tasks:
  - name: install packages for hadoop
    apt:
      pkg: 
#      - pdsh
      - acl
      - gpg-agent
      - ca-certificates
      - python3-pip
      - default-jdk 
      - default-jre
      update_cache: true
#  roles:
#    - role: geerlingguy.java
#    - role: andrewrothstein.hadoop
#      hadoop_version: 3.3.1
#  tasks:
#  - name: Copy file with owner and permissions 1
#    ansible.builtin.copy:
#      src: ./receive.py
#      dest: /root/receive.py
#      owner: root
#      group: root
#      mode: '0777'
#  - name: Copy file with owner and permissions 2
#    ansible.builtin.copy:
#      src: ./send.py
#      dest: /root/send.py
#      owner: root
#      group: root
#      mode: '0777'
#  - name: create user hadoop
#    ansible.builtin.user:
#      name: hadoop
#      shell: /bin/bash
#      groups: sudo
#      append: yes
#      generate_ssh_key: yes
#      ssh_key_bits: 2048
#      ssh_key_file: .ssh/id_rsa
#      create_home: true
#      state: present
#
#  - name: Allow 'hadoop' group to have passwordless sudo
#    lineinfile:
#      dest: /etc/sudoers
#      state: present
#      regexp: '^%wheel'
#      line: 'hadoop ALL=(ALL) NOPASSWD: ALL'
#      validate: 'visudo -cf %s'
#
#  - name: Copy id_rsa.pub to other hosts
#    lineinfile:
#      dest: /etc/sudoers
#      state: present
#      regexp: '^%wheel'
#      line: 'hadoop ALL=(ALL) NOPASSWD: ALL'
#      validate: 'visudo -cf %s'

  - name: Download hadoop distributive 
    get_url:
      url: https://downloads.apache.org/hadoop/common/hadoop-3.3.1/hadoop-3.3.1.tar.gz
      dest: /home/hadoop/hadoop-3.3.1.tar.gz
      mode: 0644

  - name: unarchiving hadoop-3.3.1.tar.gz 
    unarchive:
      remote_src: yes
      src: /home/hadoop/hadoop-3.3.1.tar.gz
      dest: /usr/local
      owner: hadoop

  - name: rename hadoop directory
    copy:
      remote_src: yes
      src: /usr/local/hadoop-3.3.1/
      dest: /usr/local/hadoop
      owner: hadoop
      group: hadoop
      mode: '0755'


#  - name: Create a directory /usr/local/hadoop/logs
#    become: yes
#    ansible.builtin.file:
#      path: /usr/local/hadoop/logs
#      state: directory
#      mode: '0755'
#      owner: hadoop
#      group: hadoop
#
#  - name: Add a line to a file if the file does not exist, without passing regexp
#    become: yes
#    become_user: hadoop
#    ansible.builtin.lineinfile:
#      path: /home/hadoop/.bashrc
#      line: "{{ item }}"
#      create: yes
#    with_items:
#       - export HADOOP_HOME=/usr/local/hadoop
#       - export HADOOP_INSTALL=$HADOOP_HOME
#       - export HADOOP_MAPRED_HOME=$HADOOP_HOME
#       - export HADOOP_COMMON_HOME=$HADOOP_HOME
#       - export HADOOP_HDFS_HOME=$HADOOP_HOME
#       - export YARN_HOME=$HADOOP_HOME
#       - export HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/native
#       - export PATH=$PATH:$HADOOP_HOME/sbin:$HADOOP_HOME/bin
#       - export HADOOP_OPTS="-Djava.library.path=$HADOOP_HOME/lib/native"
#       - export DERBY_HOME=/usr/local/derby
#       - export PATH=$PATH:$DERBY_HOME/bin
#       - export CLASSPATH=$CLASSPATH:$DERBY_HOME/lib/derby.jar:$DERBY_HOME/lib/derbytools.jar
#
#  - name: Add a line to a file if the file does not exist, without passing regexp JAVA_HOME
#    become_user: hadoop
#    ansible.builtin.lineinfile:
#      path: /usr/local/hadoop/etc/hadoop/hadoop-env.sh
#      line: "{{ item }}"
#      create: yes
#    with_items:
#       - export JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64 
#       - export HADOOP_CLASSPATH+=" $HADOOP_HOME/lib/*.jar"
#
#
#
#  - name: Download hadoop distributive 
#    get_url:
#      url: https://jcenter.bintray.com/javax/activation/javax.activation-api/1.2.0/javax.activation-api-1.2.0.jar
#      dest: /usr/local/hadoop/lib/javax.activation-api-1.2.0.jar
#      mode: 0755
#      owner: hadoop
#      group: hadoop
#
#  - name: Copy SSH Public Key in Target
#    authorized_key:
#      user: "{{ ssh_authorize_user_target_user }}"
#      key: "{{ source_user_ssh_public_key }}"
#
#  - name: Install SSH Private Key in Target
#    ansible.builtin.copy:
#      content: "{{ source_user_ssh_private_key }}"
#      dest: /home/hadoop/.ssh/id_rsa
#      owner: hadoop
#      group: hadoop
#      mode: '0600' 
#    when: ansible_facts['hostname'] == "host1"
#
#  - name: Install SSH Public Key in Target
#    ansible.builtin.copy:
#      content: "{{ source_user_ssh_public_key }}"
#      dest: /home/hadoop/.ssh/id_rsa.pub
#      owner: hadoop
#      group: hadoop
#      mode: '0644' 
#    when: ansible_facts['hostname'] == "host1"
#
#  - name: Template a files to /usr/local/hadoop/etc/hadoop/...
#    ansible.builtin.template:
#      src: "templates/{{ item.template }}"
#      dest: "/usr/local/hadoop/etc/hadoop/{{ item.name }}"
#      owner: hadoop
#      group: hadoop
#      mode: '0755'
#    with_items:
#       - { template: 'core-site.xml.j2',   name: 'core-site.xml' }
#       - { template: 'hdfs-site.xml.j2',   name: 'hdfs-site.xml' }
#       - { template: 'mapred-site.xml.j2', name: 'mapred-site.xml' }
#       - { template: 'yarn-site.xml.j2',   name: 'yarn-site.xml' }
##       - { template: 'workers.j2',   name: 'workers' }
#
#
#  - name: Download hive distributive 
#    get_url:
#      url: https://downloads.apache.org/hive/hive-3.1.2/apache-hive-3.1.2-bin.tar.gz
#      dest: /home/hadoop/apache-hive-3.1.2.tar.gz
#      mode: 0644
#
#  - name: Add a line to a file for hive
#    become: yes
#    become_user: hadoop
#    ansible.builtin.lineinfile:
#      path: /home/hadoop/.bashrc
#      line: "{{ item }}"
#      create: yes
#    with_items:
#       - export HIVE_HOME="/usr/local/hive"
#       - export PATH=$PATH:$HIVE_HOME/bin
#
#  - name: Add a line to a file for hive
#    become: yes
#    become_user: hadoop
#    ansible.builtin.lineinfile:
#      path: /home/hadoop/.bashrc
#      line: "{{ item }}"
#      create: yes
#    with_items:
#       - export HIVE_HOME= "/usr/local/hive"
#       - export PATH=$PATH:$HIVE_HOME/bin
#
#  - name: Create a directory /usr/local/hive
#    become: yes
#    ansible.builtin.file:
#      path: /usr/local/hive
#      state: directory
#      mode: '0755'
#      owner: hadoop
#      group: hadoop
#
#  - name: unarchiving hive-3.1.2.tar.gz 
#    unarchive:
#      remote_src: yes
#      src: /home/hadoop/apache-hive-3.1.2.tar.gz
#      dest: /usr/local
#      owner: hadoop
#
#  - name: rename hive directory
#    copy:
#      remote_src: yes
#      src: /usr/local/apache-hive-3.1.2-bin/
#      dest: /usr/local/hive
#      owner: hadoop
#      group: hadoop
#      mode: '0755'
#
#  - name: Add a line to a file for hive $HIVE_HOME/bin/hive-config.sh
#    become: yes
#    become_user: hadoop
#    ansible.builtin.lineinfile:
#      path: /usr/local/hive/bin/hive-config.sh
#      line: "{{ item }}"
#      create: yes
#    with_items:
#       - export HADOOP_HOME=/usr/local/hadoop/
#
#  - name: Download derby distributive 
#    get_url:
#      url: https://dlcdn.apache.org//db/derby/db-derby-10.15.2.0/db-derby-10.15.2.0-bin.tar.gz
#      dest: /home/hadoop/db-derby-10.15.2.0-bin.tar.gz
#      mode: 0644
#
#  - name: unarchiving db-derby-10.15.2.0-bin.tar.gz
#    unarchive:
#      remote_src: yes
#      src: /home/hadoop/db-derby-10.15.2.0-bin.tar.gz
#      dest: /usr/local
#      owner: hadoop
#
#  - name: rename derby directory
#    copy:
#      remote_src: yes
#      src: /usr/local/db-derby-10.15.2.0-bin/
#      dest: /usr/local/derby
#      owner: hadoop
#      group: hadoop
#      mode: '0755'
#
#  - name: Template a files to /usr/local/hive/conf/...
#    ansible.builtin.template:
#      src: "templates/{{ item.template }}"
#      dest: "/usr/local/hive/conf/{{ item.name }}"
#      owner: hadoop
#      group: hadoop
#      mode: '0755'
#    with_items:
#       - { template: 'hive-default.xml.template.j2',   name: 'hive-default.xml' }
#       - { template: 'jpox.properties.j2',   name: 'jpox.properties' }
#
#
#
#####
#####hdfs dfs -mkdir /tmp
#####hdfs dfs -chmod g+w /tmp
#####
#####hdfs dfs -mkdir -p /user/hive/warehouse
#####hdfs dfs -chmod g+w /user/hive/warehouse
#####hdfs dfs -ls /user/hive
#####
#####
#####
#####$HIVE_HOME/bin/schematool -dbType derby -initSchema
#
#  - name: unarchiving hadoop-3.3.1.tar.gz 
#    unarchive:
#      remote_src: yes
#      src: /home/hadoop/hadoop-3.3.1.tar.gz
#      dest: /usr/local
#      owner: hadoop
#
#  - name: rename hadoop directory
#    copy:
#      remote_src: yes
#      src: /usr/local/apache-hive-3.1.2-bin/
#      dest: /usr/local/hadoop
#      owner: hadoop
#      group: hadoop
#      mode: '0755'
#
#
#  - name: Create a directory /usr/local/hadoop/logs
#    become: yes
#    ansible.builtin.file:
#      path: /usr/local/hadoop/logs
#      state: directory
#      mode: '0755'
#      owner: hadoop
#      group: hadoop




  post_tasks:
  - name: Edit /etc/hosts
    lineinfile:
      path: /etc/hosts
      line: "{{ item }}"
      create: yes
    with_items:
       - 192.168.53.103 host3
       - 192.168.53.102 host2
       - 192.168.53.101 host1

  - name: Edit /etc/hosts
    lineinfile:
      path: /etc/hosts
      line: "{{ item }}"
      create: yes
    with_items:
       - 192.168.53.103 host3
       - 192.168.53.102 host2
       - 192.168.53.101 host1

  - name: comment out hosts entries for localhost
    replace:
      path: /etc/hosts
      regexp: "{{ item }}"
      replace: "# {{ item }}"
    with_items:
       - 127.0.2.1 host1 host1
       - 127.0.2.1 host2 host2
       - 127.0.2.1 host3 host3

#As hadoop user
#hdfs namenode -format
#start-dfs.sh
#start-yarn.sh
#
#
#hdfs dfs -mkdir /tmp
#hdfs dfs -chmod g+w /tmp
#
#hdfs dfs -mkdir -p /user/hive/warehouse
#hdfs dfs -chmod g+w /user/hive/warehouse
#hdfs dfs -ls /user/hive
#
#
#
#$HIVE_HOME/bin/schematool -dbType derby -initSchema
#

