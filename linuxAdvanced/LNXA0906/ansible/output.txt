


Задание:

1. Настройте Ceph кластер из двух OSD дисков, расположенных на локальном и удаленном серверах.
2. Выполните команды ceph health и ceph -s на удаленном сервере.
3. Добавьте в кластер еще по одному диску с локального и удаленного серверов.
4. Выполните команды ceph health и ceph -s на удаленном сервере.
5. Выставите размер пулов из команды ceph osd pool stats, равный двум. Выведите результат ceph -s.
6. Создайте пул rebrainme.
7. Запишите файл ceph-rebrainme-4M размером 4MiB и файл ceph-rebrainme-44M размером 44MiB в пул rebrainme.
8. Выведите результат команды rados df.






wget -q -O- 'https://download.ceph.com/keys/release.asc' | sudo apt-key add -
echo deb https://download.ceph.com/debian-pacific/ $(lsb_release -sc) main | sudo tee /etc/apt/sources.list.d/ceph.list


sudo apt update
sudo apt install ca-certificates -y
sudo apt-get install python3-setuptools
sudo pip3 install git+https://github.com/ceph/ceph-deploy.git
sudo apt install ceph-deploy -y
#The admin node must have password-less SSH access to Ceph nodes. When ceph-deploy logs in to a Ceph node as a user, that particular user must have passwordless sudo privileges.


#ssh-keygen and passwordless access to node02
Host node01
   Hostname node01
   User root
Host node02
   Hostname node02
   User root


mkdir my-cluster
cd my-cluster



ceph-deploy new {initial-monitor-node(s)}

ceph-deploy new node01
#install ceph packages
ceph-deploy install node01 node02
ceph-deploy mon create-initial
ceph-deploy admin node01
ceph-deploy mgr create node01
ceph-deploy osd create --data /dev/vdb node01
ceph-deploy osd create --data /dev/vdb node02

#Check your cluster’s health.
ssh node1 sudo ceph -s



